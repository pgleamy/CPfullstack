This is just random text to test the embedding and retrieval functions.

mport os
import pickle
import torch
from transformers import AutoModel, AutoTokenizer
import faiss
from transformers import logging
import numpy as np
import spacy
from datetime import datetime
import string
import sqlite3
from modules.binary_search import get_full_text

## Index file 
db_path = os.path.join(os.getcwd(), 'database', 'full_text_store.db')

logging.set_verbosity_error()  # to only display error messages
nlp = spacy.load("en_core_web_sm")

def query_index(index_filename: str, query_text: str, top_k=2):
    if not os.path.exists(index_filename):
        print("Index file does not exist.")
        return

    index = faiss.read_index(index_filename)
    with open(f"{index_filename}_metadata_mapping.pkl", "rb") as f:
        metadata_mapping = pickle.load(f)

    model = AutoModel.from_pretrained("bert-large-uncased")
    tokenizer = AutoTokenizer.from_pretrained("bert-large-uncased")
    
    # breaks query_text into sentences and stores in results
    doc = nlp(query_text)
    sentences = [sent.text for sent in doc.sents]
    relevant_vectors = []

    for sentence in sentences:
        input_ids = tokenizer(sentence, return_tensors="pt", truncation=True, max_length=128)["input_ids"]

        with torch.no_grad():
            outputs = model(input_ids)
        embeddings = outputs.last_hidden_state
        vector = embeddings.mean(dim=1).numpy().squeeze()
        vector = vector.reshape(-1)
        if vector.shape[0] != index.d:
            vector = vector[:index.d]

        D, I = index.search(vector.reshape(1, -1), top_k)
        for i in range(I.shape[1]):
            idx = I[0, i]
            metadata = metadata_mapping.get(idx, {})
            relevant_vectors.append(metadata)

    # Extract unique block_ids and sort them from highest to lowest
    block_ids = list(set([metadata['block_id'] for metadata in relevant_vectors]))
    block_ids.sort(reverse=True)

    return block_ids
